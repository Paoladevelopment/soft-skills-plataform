"""
Challenge evaluation module for evaluating player answers using LLM.

Provides functions to evaluate answers for different play modes using LLM-based evaluation.
"""

import json
from typing import Dict, Any
from .models import LLMClient
from .schemas import ClarifyEvaluationResponse, SummarizeEvaluationResponse, ParaphraseEvaluationResponse
from .prompt_loader import PromptLoader
from .prompt_builder import PromptBuilder
from utils.errors import APIException


def evaluate_clarify_questions(
    reference_questions: list[str],
    player_questions: list[str],
    model_name: str = "gpt-4o-mini",
    temperature: float = 0.1
) -> ClarifyEvaluationResponse:
    """
    Evaluate clarifying questions using LLM.
    
    Args:
        reference_questions: List of reference clarifying questions from the challenge.
        player_questions: List of player-submitted clarifying questions to evaluate.
        model_name: OpenAI model name (default: gpt-4o-mini).
        temperature: Model temperature (default: 0.1).
        
    Returns:
        ClarifyEvaluationResponse with evaluations for each question.
    """
    try:
        prompt_loader = PromptLoader()
        prompt_builder = PromptBuilder(prompt_loader)
        
        llm_client = LLMClient(model_name=model_name, temperature=temperature)
        prompt_template = prompt_builder.build_evaluation_prompt("clarify")
        
        evaluation = llm_client.execute_prompt(
            prompt_template,
            ClarifyEvaluationResponse,
            reference_questions=json.dumps(reference_questions, ensure_ascii=False),
            player_questions=json.dumps(player_questions, ensure_ascii=False)
        )
        
        if len(evaluation.per_question) != len(player_questions):
            raise APIException(
                f"LLM returned {len(evaluation.per_question)} evaluations for {len(player_questions)} questions"
            )
        
        return evaluation
        
    except APIException:
        raise
    except Exception as e:
        raise APIException(f"Failed to evaluate clarifying questions: {str(e)}")


def evaluate_summarize_answer(
    reference_summary: str,
    player_summary: str,
    model_name: str = "gpt-4o-mini",
    temperature: float = 0.1
) -> SummarizeEvaluationResponse:
    """
    Evaluate a summarize answer using LLM.
    
    Args:
        reference_summary: Reference summary from the challenge.
        player_summary: Player-submitted summary to evaluate.
        model_name: OpenAI model name (default: gpt-4o-mini).
        temperature: Model temperature (default: 0.1).
        
    Returns:
        SummarizeEvaluationResponse with evaluation.
    """
    try:
        llm_client = LLMClient(model_name=model_name, temperature=temperature)
        prompt_loader = PromptLoader()
        prompt_builder = PromptBuilder(prompt_loader)
        prompt_template = prompt_builder.build_evaluation_prompt("summarize")
        
        evaluation = llm_client.execute_prompt(
            prompt_template,
            SummarizeEvaluationResponse,
            reference_summary=reference_summary,
            player_summary=player_summary
        )
        
        return evaluation
        
    except APIException:
        raise
    except Exception as e:
        raise APIException(f"Failed to evaluate summarize answer: {str(e)}")


def evaluate_paraphrase_answer(
    reference_text: str,
    player_paraphrase: str,
    rubric: list[str],
    model_name: str = "gpt-4o-mini",
    temperature: float = 0.1
) -> ParaphraseEvaluationResponse:
    """
    Evaluate a paraphrase answer using LLM.
    
    Args:
        reference_text: Reference paraphrase generated by LLM when creating the challenge. 
                        This is NOT the original audio content, but rather an example paraphrase.
        player_paraphrase: Player-submitted paraphrase to evaluate.
        rubric: List of evaluation criteria for the paraphrase.
        model_name: OpenAI model name (default: gpt-4o-mini).
        temperature: Model temperature (default: 0.1).
        
    Returns:
        ParaphraseEvaluationResponse with evaluation.
    """
    try:
        llm_client = LLMClient(model_name=model_name, temperature=temperature)
        prompt_loader = PromptLoader()
        prompt_builder = PromptBuilder(prompt_loader)
        prompt_template = prompt_builder.build_evaluation_prompt("paraphrase")
        
        evaluation = llm_client.execute_prompt(
            prompt_template,
            ParaphraseEvaluationResponse,
            reference_text=reference_text,
            player_paraphrase=player_paraphrase,
            rubric=json.dumps(rubric, ensure_ascii=False)
        )
        
        return evaluation
        
    except APIException:
        raise
    except Exception as e:
        raise APIException(f"Failed to evaluate paraphrase answer: {str(e)}")

